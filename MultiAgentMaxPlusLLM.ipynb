{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "k04tkJ6_fjQb",
        "WQxAq_iifxwt",
        "sONbGq0pozx-"
      ],
      "authorship_tag": "ABX9TyPmTt+L63CfvMN92btx8vrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shushman/jkg-shushman/blob/main/MultiAgentMaxPlusLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "This notebook illustrates ideas from scalable anytime multi-agent coordination to two LLM-based settings:\n",
        "\n",
        "1.  **Instruction Tuning (Meta-Prompting):** This section implements a meta-prompting framework where multiple LLM agents collaborate in a coordination graph to propose, refine, and evaluate candidate prompts.\n",
        "\n",
        "2.  **Adaptive Divide-and-Conquer Arithmetic:** This section demonstrates a strategy for solving complex multi-step arithmetic problems by adaptively breaking them down into smaller, binary sub-expressions. Specialized agents then solve these sub-problems in parallel, and the results are progressively substituted back into the main expression until a final answer is reached. This approach aims to handle complex arithmetic in an interruptible and potentially more robust manner.\n",
        "\n",
        "The notebook includes code for setting up the environment, loading and preparing datasets, defining the necessary DSPy modules for each method, and running experiments to evaluate their performance."
      ],
      "metadata": {
        "id": "-QcqpvQA61kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sec 1: Setup and Imports\n",
        "\n",
        "This section configures API keys, imports necessary libraries, and defines our large language model (LLM) wrappers and datasets."
      ],
      "metadata": {
        "id": "tx-9W8obdYBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Configuration"
      ],
      "metadata": {
        "id": "IUd2rm7DdkR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U dspy==2.6.10 litellm"
      ],
      "metadata": {
        "id": "ghbWNmYDeKG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YnabmqwKDxM"
      },
      "outputs": [],
      "source": [
        "# Configure API keys from Colab user data\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openai_api_key\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"anthropic_api_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "3m8tdDgNdtXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Numerical utilities\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# DSPy framework and evaluation\n",
        "import dspy\n",
        "from dspy.datasets import DataLoader, MATH\n",
        "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
        "from dspy.evaluate import Evaluate\n",
        "from litellm import completion\n",
        "import textwrap\n",
        "\n",
        "dspy.configure(experimental=True)"
      ],
      "metadata": {
        "id": "laftQKj0dov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Datasets"
      ],
      "metadata": {
        "id": "S5wqBc0RfWCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [GSM8K](https://huggingface.co/datasets/openai/gsm8k)\n",
        "\n",
        "Natively available in DSPy already"
      ],
      "metadata": {
        "id": "k04tkJ6_fjQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GSM8K math word problems\n",
        "gsm8k = GSM8K()\n",
        "gsm8k_train, gsm8k_dev, gsm8k_test = gsm8k.train, gsm8k.dev, gsm8k.test\n",
        "gsm8k_train_dev = gsm8k_train + gsm8k_dev"
      ],
      "metadata": {
        "id": "cllduaP7fpx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MetaPrompting Datasets\n",
        "\n",
        "We load several datasets from the [Meta-Prompting](https://github.com/suzgunmirac/meta-prompting) benchmark and split them into train/dev."
      ],
      "metadata": {
        "id": "WQxAq_iifxwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to load and split datasets\n",
        "dl = DataLoader()\n",
        "meta_prompter = dl.from_huggingface(\"turingmachine/meta-prompting\")\n",
        "\n",
        "def preprocess_metaprompter_dataset(ds_name: str, test_size: float = 0.5):\n",
        "    \"\"\"\n",
        "    Load a MetaPrompter dataset and split into train/dev.\n",
        "    Sets input keys for DSPy compatibility.\n",
        "    \"\"\"\n",
        "    ds = meta_prompter[ds_name]\n",
        "    for example in ds:\n",
        "        example._input_keys = {\"input\"}\n",
        "    train, dev = train_test_split(ds, test_size=test_size, random_state=42)\n",
        "    return train, dev\n",
        "\n",
        "# Define metric for MetaPrompter tasks\n",
        "def meta_prompter_metric(example, response, trace=None) -> int:\n",
        "    return int(example.target == response.answer)"
      ],
      "metadata": {
        "id": "-zDeUjAJftl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric Shapes and Word Sorting**"
      ],
      "metadata": {
        "id": "5amZCaRLf8VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geom_shapes_train, geom_shapes_dev = preprocess_metaprompter_dataset(\"GeometricShapes\")\n",
        "word_sorting_train, word_sorting_dev = preprocess_metaprompter_dataset(\"WordSorting\")"
      ],
      "metadata": {
        "id": "cb4sYulDf-MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Step Arithmetic and Extended Combinations**"
      ],
      "metadata": {
        "id": "IF9jdZlOf_uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_step_train, multi_step_dev = preprocess_metaprompter_dataset(\"MultistepArithmeticTwo\")\n",
        "multi_step_combined = multi_step_train + multi_step_dev\n",
        "\n",
        "def combine_multi_step_arithmetic(ex1, ex2):\n",
        "    \"\"\"\n",
        "    Combine two arithmetic examples into a single multiplication problem.\n",
        "    \"\"\"\n",
        "    new_input = f\"{ex1.input.rstrip(' ?')} * {ex2.input.rstrip(' ?')}\"\n",
        "    new_target = str(int(ex1.target) * int(ex2.target))\n",
        "    return dspy.Example(input=new_input, target=new_target)\n",
        "\n",
        "# Generate extended dataset by chaining multiplications\n",
        "multi_step_extended = [\n",
        "    combine_multi_step_arithmetic(multi_step_combined[i], multi_step_combined[i+1])\n",
        "    for i in range(len(multi_step_combined) - 1)\n",
        "]\n",
        "\n",
        "# Peek at an example\n",
        "multi_step_extended[5]"
      ],
      "metadata": {
        "id": "3gxJwYhCgBUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sec 2: Instruction Tuning\n",
        "This section implements a meta‑prompting framework where multiple LLM agents propose, refine, and evaluate candidate prompts in a coordination graph. We omit file writes and simply print metrics for downstream adjustment."
      ],
      "metadata": {
        "id": "aHICoo_C3qNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Library Modules"
      ],
      "metadata": {
        "id": "1y8xXIkF4Be4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Base LLM Signatures and Wrappers\n",
        "We define DSPy signatures and modules for each task type (GSM8K, GeometricShapes, WordSorting, MultiStepArithmetic)."
      ],
      "metadata": {
        "id": "N30LfmqT4DyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GSM8K Signature & Module\n",
        "class GSM8kSignature(dspy.Signature):\n",
        "    question: str = dspy.InputField(desc=\"math word problem\")\n",
        "    reasoning: str = dspy.OutputField(desc=\"step-by-step reasoning\")\n",
        "    answer: int = dspy.OutputField(desc=\"integer answer\")\n",
        "\n",
        "class GSM8kModule(dspy.Module):\n",
        "    def __init__(self, llm):\n",
        "        self.predictor = dspy.Predict(GSM8kSignature)\n",
        "        self.predictor.set_lm(llm)\n",
        "\n",
        "    def set_instructions(self, instr: str):\n",
        "        self.predictor.signature = GSM8kSignature.with_instructions(instr)\n",
        "\n",
        "    def __call__(self, question: str) -> dspy.Prediction:\n",
        "        resp = self.predictor(question=question)\n",
        "        return dspy.Prediction(reasoning=resp.reasoning, answer=str(resp.answer))\n",
        "\n",
        "# GeometricShapes Signature & Module\n",
        "class GeomShapesSignature(dspy.Signature):\n",
        "    input: str = dspy.InputField(desc=\"SVG path question\")\n",
        "    reasoning: str = dspy.OutputField(desc=\"chain-of-thought reasoning\")\n",
        "    answer: str = dspy.OutputField(desc=\"shape label in parentheses\")\n",
        "\n",
        "class GeomShapesModule(dspy.Module):\n",
        "    def __init__(self, llm):\n",
        "        self.predictor = dspy.Predict(GeomShapesSignature)\n",
        "        self.predictor.set_lm(llm)\n",
        "\n",
        "    def set_instructions(self, instr: str):\n",
        "        self.predictor.signature = GeomShapesSignature.with_instructions(instr)\n",
        "\n",
        "    def __call__(self, input: str) -> dspy.Prediction:\n",
        "        resp = self.predictor(input=input)\n",
        "        # Truncate answer to first option label\n",
        "        return dspy.Prediction(reasoning=resp.reasoning, answer=resp.answer[:3])\n",
        "\n",
        "# MultiStepArithmetic Signature & Module\n",
        "class MultiStepSignature(dspy.Signature):\n",
        "    input: str = dspy.InputField(desc=\"arithmetic problem\")\n",
        "    reasoning: str = dspy.OutputField(desc=\"chain-of-thought\")\n",
        "    answer: int = dspy.OutputField(desc=\"integer result\")\n",
        "\n",
        "class MultiStepModule(dspy.Module):\n",
        "    def __init__(self, llm):\n",
        "        self.predictor = dspy.Predict(MultiStepSignature)\n",
        "        self.predictor.set_lm(llm)\n",
        "\n",
        "    def set_instructions(self, instr: str):\n",
        "        self.predictor.signature = MultiStepSignature.with_instructions(instr)\n",
        "\n",
        "    def __call__(self, input: str) -> dspy.Prediction:\n",
        "        resp = self.predictor(input=input)\n",
        "        return dspy.Prediction(reasoning=resp.reasoning, answer=str(resp.answer))"
      ],
      "metadata": {
        "id": "O-WSN_GxgHLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Proposer & Refiner\n",
        "Two cooperating modules generate and refine candidate prompts via message passing."
      ],
      "metadata": {
        "id": "xVyevJH35GyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposalAgentSignature(dspy.Signature):\n",
        "  task_description: str = dspy.InputField(desc=\"The task the prompt should address.\")\n",
        "  prompt: str = dspy.OutputField(desc=\"The generated candidate prompt.\")\n",
        "\n",
        "class RefinerSignature(dspy.Signature):\n",
        "  task_description: str = dspy.InputField(desc=\"The task the prompt should address.\")\n",
        "  current_prompt: str = dspy.InputField(desc=\"The agent's current candidate prompt.\")\n",
        "  neighbor_prompts: list[str] = dspy.InputField(desc=\"The current prompts of the agent's neighbors in the coordination graph.\")\n",
        "  feedback_to_refine_prompt: str = dspy.InputField(desc=\"Feedback for refining the agent's current prompt.\")\n",
        "  refined_prompt: str = dspy.OutputField(desc=\"The improved candidate prompt after refinement.\")\n",
        "\n",
        "class ProposalAgent(dspy.Module):\n",
        "  def __init__(self, idx, task_description, llm, verbose=False):\n",
        "    self.task_description = task_description\n",
        "    self.verbose = verbose\n",
        "    self.idx = idx\n",
        "\n",
        "    self.proposal_instr = \"\"\"\n",
        "    You are an expert prompt generator. Craft a creative and effective prompt for a large language model that\n",
        "    will improve its performance at solving the given described task.\n",
        "    \"\"\"\n",
        "    self.refinement_instr = \"\"\"\n",
        "    You are an expert prompt editor.\n",
        "    Refine the current task prompt based on the feedback and on the prompts of your coordinating neighbors.\n",
        "    Ensure the refined prompot is effective at solving the described task.\n",
        "    \"\"\"\n",
        "    prop_signature = ProposalAgentSignature.with_instructions(self.proposal_instr)\n",
        "    ref_signature = RefinerSignature.with_instructions(self.refinement_instr)\n",
        "    self.generate_module = dspy.Predict(prop_signature)\n",
        "    self.refine_module = dspy.Predict(ref_signature)\n",
        "\n",
        "    self.current_prompt = \"\"\n",
        "    self.current_score = 0.0\n",
        "    self.set_lm(llm)\n",
        "\n",
        "  def generate_initial_prompt(self):\n",
        "    \"\"\"\n",
        "    Generate the initial prompt for the task.\n",
        "    \"\"\"\n",
        "    self.current_prompt = self.generate_module(\n",
        "        task_description=self.task_description).prompt\n",
        "\n",
        "  def refine_prompt(self, neighbor_prompts, feedback):\n",
        "    \"\"\"\n",
        "    Refine the agent's current prompt based on its score and neighbor information.\n",
        "    \"\"\"\n",
        "    self.current_prompt = self.refine_module(\n",
        "      task_description=self.task_description,\n",
        "      current_prompt=self.current_prompt,\n",
        "      neighbor_prompts=neighbor_prompts,\n",
        "      feedback_to_refine_prompt=feedback\n",
        "    ).refined_prompt"
      ],
      "metadata": {
        "id": "VCOdrqJb5A7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Evaluator"
      ],
      "metadata": {
        "id": "oF9Ft1EI5Uvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Evaluator Agent; just a wrapper around dspy.Evaluate\n",
        "class EvaluatorAgent:\n",
        "  def __init__(self, model_lm, devset, metric, minibatch, **eval_kwargs):\n",
        "    \"\"\"\n",
        "    Initialize the EvaluatorAgent.\n",
        "\n",
        "    Args:\n",
        "      model_lm (dspy.LM): The base language model for evaluation.\n",
        "      devset (list): The full development dataset.\n",
        "      metric (str): Metric to evaluate the prompts (e.g., accuracy, BLEU).\n",
        "      minibatch (int): Number of samples to use for each evaluation.\n",
        "      **eval_kwargs: Additional arguments for dspy.Evaluate.\n",
        "    \"\"\"\n",
        "    self.model_lm = model_lm\n",
        "    self.eval_kwargs = eval_kwargs\n",
        "    self.devset = devset\n",
        "    self.metric = metric\n",
        "    self.minibatch = minibatch\n",
        "\n",
        "    # Ensure subset size is valid\n",
        "    assert self.minibatch <= len(self.devset), \"Subset size exceeds dataset size.\"\n",
        "\n",
        "    self.current_subset = None  # Placeholder for the subset used in a round\n",
        "\n",
        "  def set_subset_for_round(self):\n",
        "    \"\"\"\n",
        "    Set a random subset of the dataset to be used for the current round.\n",
        "    \"\"\"\n",
        "    self.current_subset = random.sample(self.devset, self.minibatch)\n",
        "\n",
        "  def set_full_set(self):\n",
        "    self.current_subset = self.devset\n",
        "\n",
        "  def evaluate_prompt(self, candidate_prompt):\n",
        "    \"\"\"\n",
        "    Evaluate a candidate prompt using the model and the current subset of the development dataset.\n",
        "\n",
        "    Args:\n",
        "      candidate_prompt (str): The prompt to evaluate.\n",
        "\n",
        "    Returns:\n",
        "      float: The evaluation score of the prompt.\n",
        "    \"\"\"\n",
        "    # Ensure the subset is set\n",
        "    if self.current_subset is None:\n",
        "      raise ValueError(\"Dataset subset for the current round has not been set. Call set_subset_for_round() first.\")\n",
        "\n",
        "    # Create the evaluator\n",
        "    evaluator = dspy.Evaluate(\n",
        "        devset=self.current_subset, metric=self.metric, **self.eval_kwargs\n",
        "    )\n",
        "\n",
        "    # Define the model to evaluate with the given prompt\n",
        "    self.model_lm.set_instructions(candidate_prompt)\n",
        "\n",
        "    # Evaluate the model and return the score\n",
        "    score = evaluator(self.model_lm)\n",
        "    return score"
      ],
      "metadata": {
        "id": "vvxUIZUq5WJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Coordination Graph & Orchestrator\n",
        "Defines the message‑passing algorithm over multiple rounds."
      ],
      "metadata": {
        "id": "sA7QBE7E5Z68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoordinationGraphSignature(dspy.Signature):\n",
        "  task_description: str = dspy.InputField(desc=\"The task the prompts should address.\")\n",
        "  agent_prompts: list[str] = dspy.InputField(desc=\"List of all current prompts from the agents.\")\n",
        "  previous_round_scores: list[float] = dspy.InputField(desc=\"List of prompt scores from the previous round.\")\n",
        "  feedback_to_refine_prompts: list[str] = dspy.OutputField(desc=\"Feedback for refining each agent's prompt.\")\n",
        "  coordination_graph: list[list[int]] = dspy.OutputField(\n",
        "      desc=\"For each agent in order, a list of other agents it should coordinate prompts with.\")\n",
        "\n",
        "# Define Coordination Graph and Message Passing\n",
        "class CoordinationGraph:\n",
        "  def __init__(self, proposal_agents, task_description, llm, max_neighbors=3, verbose=False):\n",
        "    \"\"\"\n",
        "    Initialize the CoordinationGraph with a list of proposal agents, task description,\n",
        "    and an LLM for generating the graph.\n",
        "\n",
        "    Args:\n",
        "      proposal_agents (list): List of ProposalAgent objects.\n",
        "      task_description (str): Description of the task the prompts are addressing.\n",
        "      llm (dspy.LM): The language model to generate the coordination graph.\n",
        "      verbose (bool): Whether to print verbose output.\n",
        "    \"\"\"\n",
        "    self.proposal_agents = proposal_agents\n",
        "    self.task_description = task_description\n",
        "    self.verbose = verbose\n",
        "\n",
        "    self.graph = {agent.idx: [] for agent in proposal_agents}\n",
        "    self.current_refinement_feedback = []\n",
        "\n",
        "    # Define the DSPy module for generating the coordination graph\n",
        "    self.graph_instr = f\"\"\"\n",
        "      You are an expert at prompt writing and editing.\n",
        "      Based on the task description, current agent prompts, and corresponding scores in the previous\n",
        "      round, determine which agents should coordinate their prompts in the next round to improve their\n",
        "      performance on the described task.\n",
        "      Consider the following principles for deciding which agents should coordinate:\n",
        "      1. Agents with very similar prompts should coordinate to avoid redundancy.\n",
        "      2. If a pair of agents have different prompts and different scores,\n",
        "      the lower scoring agent should coordinate with the higher scoring agent but not vice versa.\n",
        "      3. The highest scoring agents don't necessarily have to coordinate but should still\n",
        "      get feedback on possible improvements.\n",
        "      Limit the number of such neighbors for each agent to a maximum of {max_neighbors}, prioritizing the most relevant dependencies.\n",
        "      Return a mapping of each agent to the agents it should coordinate with and\n",
        "      specific and targeted feedback for each agent on how they should refine their prompts to improve\n",
        "      performance on the described task.\n",
        "    \"\"\"\n",
        "    cg_signature = CoordinationGraphSignature.with_instructions(self.graph_instr)\n",
        "    self.graph_module = dspy.Predict(cg_signature)\n",
        "\n",
        "    self.graph_module.set_lm(llm)\n",
        "    self.max_neighbors = max_neighbors\n",
        "\n",
        "  def update_graph(self, previous_round_scores):\n",
        "    \"\"\"\n",
        "    Update the coordination graph using the LLM to evaluate relationships between prompts.\n",
        "\n",
        "    Args:\n",
        "      max_neighbors (int): Maximum number of neighbors for each agent.\n",
        "    \"\"\"\n",
        "    # Gather current prompts from all agents\n",
        "    prompts = [agent.current_prompt for agent in self.proposal_agents]\n",
        "    idxs = [agent.idx for agent in self.proposal_agents]\n",
        "\n",
        "    # Use the LLM to generate the coordination graph\n",
        "    result = self.graph_module(\n",
        "        task_description=self.task_description,\n",
        "        agent_prompts=prompts,\n",
        "        previous_round_scores=previous_round_scores\n",
        "    )\n",
        "\n",
        "    # Limit the number of neighbors per agent\n",
        "    self.graph = {}\n",
        "    for (i, neighbors) in enumerate(result.coordination_graph):\n",
        "        # Prioritize neighbors based on some criteria (e.g., relevance, score, etc.)\n",
        "        prioritized_neighbors = neighbors[:self.max_neighbors]\n",
        "        self.graph[idxs[i]] = prioritized_neighbors\n",
        "        if self.verbose:\n",
        "          print(f\"Agent {idxs[i]}: Neighbors -> {prioritized_neighbors}\")\n",
        "\n",
        "    # Set current refinement feedback\n",
        "    self.current_refinement_feedback = result.feedback_to_refine_prompts\n",
        "    if self.verbose:\n",
        "      print(\"Refinement Feedback:\")\n",
        "      for idx, feedback in enumerate(self.current_refinement_feedback):\n",
        "          print(f\"Agent {idx}: Feedback -> {feedback}\")\n"
      ],
      "metadata": {
        "id": "GV1acVso5eoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Orchestrator:\n",
        "  def __init__(self, proposal_agents, evaluator, coordination_graph, num_rounds, subset=True, verbose=False):\n",
        "    \"\"\"\n",
        "    Initialize the orchestrator.\n",
        "\n",
        "    Args:\n",
        "      proposal_agents (list): List of ProposalAgent instances.\n",
        "      evaluator (EvaluatorAgent): Evaluator agent for scoring prompts.\n",
        "      coordination_graph (CoordinationGraph): Dynamic coordination graph.\n",
        "      num_rounds (int): Number of message-passing rounds.\n",
        "      verbose (bool): Whether to enable verbose output.\n",
        "    \"\"\"\n",
        "    self.proposal_agents = proposal_agents\n",
        "    self.evaluator = evaluator\n",
        "    self.coordination_graph = coordination_graph\n",
        "    self.num_rounds = num_rounds\n",
        "    self.verbose = verbose\n",
        "    self.subset = subset\n",
        "\n",
        "    # Track all scores and prompts\n",
        "    self.all_scores = []\n",
        "    self.all_prompts = []\n",
        "    self.best_score_in_round = []\n",
        "    self.best_prompt_in_round = []\n",
        "\n",
        "  def run(self):\n",
        "    # Generate initial prompts\n",
        "    print(\"\\n--- Generating and Evaluating Initial Prompts ---\")\n",
        "    for agent in self.proposal_agents:\n",
        "      agent.generate_initial_prompt()\n",
        "\n",
        "    # Initial evaluation\n",
        "    self._evaluate_all_prompts()\n",
        "\n",
        "    # Get best score and prompt after round\n",
        "    best_prompt, best_score = self._get_best_prompt_and_score_in_round()\n",
        "    self.best_prompt_in_round.append(best_prompt)\n",
        "    self.best_score_in_round.append(best_score)\n",
        "\n",
        "    # Iterative message passing and refinement\n",
        "    for round_num in range(self.num_rounds):\n",
        "      print(f\"\\n--- Round {round_num + 1} ---\")\n",
        "\n",
        "      # Update coordination graph\n",
        "      self.coordination_graph.update_graph(\n",
        "          previous_round_scores=[\n",
        "              agent.current_score for agent in self.proposal_agents\n",
        "          ]\n",
        "      )\n",
        "\n",
        "      # Refine prompts using message passing\n",
        "      for agent in self.proposal_agents:\n",
        "        neighbor_prompts = [\n",
        "            self.proposal_agents[nbr_idx].current_prompt for nbr_idx in self.coordination_graph.graph[agent.idx]]\n",
        "        feedback = self.coordination_graph.current_refinement_feedback[agent.idx]\n",
        "        agent.refine_prompt(neighbor_prompts, feedback)\n",
        "\n",
        "      # Evaluate refined prompts\n",
        "      self._evaluate_all_prompts()\n",
        "      best_prompt, best_score = self._get_best_prompt_and_score_in_round()\n",
        "      self.best_prompt_in_round.append(best_prompt)\n",
        "      self.best_score_in_round.append(best_score)\n",
        "\n",
        "      # Print the best score so far\n",
        "      if self.verbose:\n",
        "        print(f\"\\n--- Best Score: {max(self.all_scores)}\")\n",
        "\n",
        "    # Decide final output\n",
        "    print(\"\\n--- Finalizing Output ---\")\n",
        "    best_prompt, best_score = self._get_best_prompt_and_score()\n",
        "\n",
        "    final_results = {\n",
        "        \"best_prompt\": best_prompt,\n",
        "        \"best_score\": best_score,\n",
        "        \"best_prompt_after_round\": self.best_prompt_in_round,\n",
        "        \"best_score_after_round\": self.best_score_in_round\n",
        "    }\n",
        "    return final_results\n",
        "\n",
        "  def _evaluate_all_prompts(self):\n",
        "    \"\"\"\n",
        "    Evaluate all prompts and track their scores and prompts.\n",
        "    \"\"\"\n",
        "    if self.subset:\n",
        "      self.evaluator.set_subset_for_round()\n",
        "    else:\n",
        "      self.evaluator.set_full_set()\n",
        "    for agent in self.proposal_agents:\n",
        "      score = self.evaluator.evaluate_prompt(agent.current_prompt)\n",
        "      agent.current_score = score\n",
        "\n",
        "      # Track all scores and prompts\n",
        "      self.all_scores.append(score)\n",
        "      self.all_prompts.append(agent.current_prompt)\n",
        "\n",
        "      if self.verbose:\n",
        "        print(f\"Agent Prompt: {agent.current_prompt}, Score: {score}\")\n",
        "\n",
        "  def _get_best_prompt_and_score(self):\n",
        "    \"\"\"\n",
        "    Get the best prompt and its score across all rounds.\n",
        "\n",
        "    Returns:\n",
        "      tuple: Best prompt and its score.\n",
        "    \"\"\"\n",
        "    best_index = self.all_scores.index(max(self.all_scores))\n",
        "    best_prompt = self.all_prompts[best_index]\n",
        "    best_score = self.all_scores[best_index]\n",
        "    return best_prompt, best_score\n",
        "\n",
        "  def _get_best_prompt_and_score_in_round(self):\n",
        "    curr_scores = []\n",
        "    curr_prompts = []\n",
        "    for agent in self.proposal_agents:\n",
        "      curr_scores.append(agent.current_score)\n",
        "      curr_prompts.append(agent.current_prompt)\n",
        "    best_score = max(curr_scores)\n",
        "    best_index = curr_scores.index(best_score)\n",
        "    best_prompt = curr_prompts[best_index]\n",
        "    return best_prompt, best_score"
      ],
      "metadata": {
        "id": "2gDbAkczFgor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Usage"
      ],
      "metadata": {
        "id": "6tZqbbCz62kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Configure Models & Task"
      ],
      "metadata": {
        "id": "lYTQzMxi66BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LLM instances for this section\n",
        "gpt4o = dspy.LM('openai/gpt-4o', temperature=0.7, max_tokens=3000, cache=False)\n",
        "gpt4o_mini = dspy.LM('openai/gpt-4o-mini', temperature=0.7, max_tokens=3000, cache=False)\n",
        "haiku3 = dspy.LM('anthropic/claude-3-haiku-20240307', max_tokens=3000, cache=False)\n",
        "gpt_35 = dspy.LM('openai/gpt-3.5-turbo', max_tokens=3000, cache=False)\n",
        "\n",
        "# Select configurable params for experiments\n",
        "task = 'gsm8k' #@param enum=[\"geom_shapes\", \"gsm8k\", \"multi_step\"]\n",
        "n_agents = 4 #@param {type:\"integer\"}\n",
        "n_rounds = 2 #@param {type:\"integer\"}\n",
        "max_cg_nbrs = 1 #@param {type:\"integer\"}\n",
        "verbose = False #@param {type:\"boolean\"}\n",
        "\n",
        "base_map = {\n",
        "    'gsm8k': (GSM8kModule, gpt_35),\n",
        "    'multi_step': (MultiStepModule, gpt_35),\n",
        "    'geom_shapes': (GeomShapesModule, haiku3),\n",
        "}\n",
        "ModuleClass, base_llm = base_map[task]\n",
        "\n",
        "if task == \"gsm8k\":\n",
        "  trainset = gsm8k_dev\n",
        "  testset = gsm8k_test[0:1300:6]\n",
        "  metric = gsm8k_metric\n",
        "elif task == \"geom_shapes\":\n",
        "  trainset = geom_shapes_train\n",
        "  testset = geom_shapes_dev\n",
        "  metric = meta_prompter_metric\n",
        "elif task == \"multi_step\":\n",
        "  trainset = multi_step_train\n",
        "  testset = multi_step_dev\n",
        "  metric = meta_prompter_metric\n",
        "\n",
        "TASK_DESCRIPTION_MAP = {\n",
        "    \"gsm8k\": \"Solve complex mathematical reasoning problems correctly\",\n",
        "    \"multi_step\": \"Solve complex arithmetic problems correctly\",\n",
        "    \"geom_shapes\": \"Correctly identify a 2D geometric shape from an SVG path element\",\n",
        "}\n",
        "\n",
        "task_description = TASK_DESCRIPTION_MAP[task]\n",
        "\n",
        "eval_kwargs = dict(num_threads=2, display_progress=True, display_table=0)\n",
        "dspy_evaluator = dspy.Evaluate(devset=testset, metric=metric, **eval_kwargs)\n",
        "\n",
        "# Instantiate evaluation metric and evaluator\n",
        "evaluator = EvaluatorAgent(\n",
        "    model_lm=ModuleClass(base_llm),\n",
        "    devset=trainset,\n",
        "    metric=metric,\n",
        "    minibatch=50,\n",
        "    num_threads=2,\n",
        "    display_progress=True\n",
        ")"
      ],
      "metadata": {
        "id": "c0Tj457u69gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Run Multi-Agent Prompt Optimization"
      ],
      "metadata": {
        "id": "jqb80fu29d9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize proposal agents and coordination graph\n",
        "agents = [ProposalAgent(i, task_description, gpt4o_mini) for i in range(5)]\n",
        "cg = CoordinationGraph(agents, task_description, gpt4o, max_cg_nbrs, verbose)\n",
        "orch = Orchestrator(agents, evaluator, cg, num_rounds=n_rounds, subset=True, verbose=True)\n",
        "\n",
        "results = orch.run()\n",
        "print(\"Optimization Rounds → Best Scores on minibatch:\", results['best_score_after_round'])"
      ],
      "metadata": {
        "id": "8ffUfIyG9hAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Held‑Out Evaluation & Baselines\n",
        "We apply the best prompts to a held‑out test set and compare against baselines."
      ],
      "metadata": {
        "id": "bEJq9Sdn9_8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Held‑out scoring\n",
        "heldout_scores = []\n",
        "\n",
        "for i in range(n_rounds+1):\n",
        "  prompt = results['best_prompt_after_round'][i]\n",
        "  if i >=1 and prompt == results['best_prompt_after_round'][i-1]:\n",
        "    print(\"Skipping duplicate prompt\")\n",
        "    heldout_scores.append(heldout_score)\n",
        "    continue\n",
        "  maxplus_optimized_module = ModuleClass(base_llm)\n",
        "  maxplus_optimized_module.set_instructions(prompt)\n",
        "  heldout_score = dspy_evaluator(maxplus_optimized_module)\n",
        "  heldout_scores.append(heldout_score)\n",
        "print(\"Test set scores per round:\" + str(heldout_scores))\n",
        "\n",
        "# Baselines\n",
        "# 1) MIPRO\n",
        "from dspy import MIPROv2\n",
        "mipro_module_before_opt = ModuleClass(llm=base_llm)\n",
        "kwargs = dict(num_threads=2, prompt_model=gpt4o_mini, auto=\"medium\")\n",
        "optimizer = dspy.MIPROv2(metric=metric, **kwargs)\n",
        "kwargs = dict(requires_permission_to_run=False, max_bootstrapped_demos=0, max_labeled_demos=0)\n",
        "mipro_optimized_module = optimizer.compile(mipro_module_before_opt, trainset=random.sample(trainset, min(n_rounds*50, len(trainset))), **kwargs)\n",
        "\n",
        "mipro_res = dspy_evaluator(mipro_optimized_module)\n",
        "print(\"MIPRO baseline:\", mipro_res)\n",
        "\n",
        "# 2) Base LLM\n",
        "base_mod = ModuleClass(base_llm)\n",
        "print(\"Base LLM accuracy:\", dspy_evaluator(base_mod))\n",
        "\n",
        "# # 3) CoT\n",
        "gsm_cot_module = ModuleClass(llm=base_llm)\n",
        "meta_prompter_cot_module = dspy.ChainOfThought(\"input -> answer\")\n",
        "meta_prompter_cot_module.set_lm(base_llm)\n",
        "\n",
        "if task == \"gsm8k\":\n",
        "  cot_module = gsm_cot_module\n",
        "else:\n",
        "  cot_module = meta_prompter_cot_module\n",
        "print(\"Chain-of-Thought:\", dspy_evaluator(cot_module))"
      ],
      "metadata": {
        "id": "vovxTsTS-W6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sec 3: Adaptive Divide-and-Conquer Arithmetic\n",
        "This notebook section demonstrates how to split multi-step arithmetic into binary sub-expressions,\n",
        "coordinate specialized agents, and stream intermediate results in an interruptible fashion."
      ],
      "metadata": {
        "id": "gHpLutvFnPqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Library Modules"
      ],
      "metadata": {
        "id": "sONbGq0pozx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Expression Decomposer\n",
        "Breaks a complex expression into as many binary sub-expressions as possible for one round."
      ],
      "metadata": {
        "id": "e1E7ZvkGpBP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpressionDecompositionSignature(dspy.Signature):\n",
        "  full_expr: str = dspy.InputField(desc=\"The expression to decompose.\")\n",
        "  sub_expressions: list[str] = dspy.OutputField(desc=\"Binary sub-expressions to solve in parallel.\")\n",
        "  plan: str = dspy.OutputField(desc=\"Placeholder-based plan for substituting results.\")\n",
        "\n",
        "class ExpressionDecomposer:\n",
        "  def __init__(self, llm, verbose=False):\n",
        "    self.llm = llm\n",
        "    self.verbose = verbose\n",
        "    self.decomp_instructions = (\n",
        "      \"\"\"\n",
        "      You are an expert arithmetic expression analyzer.\n",
        "      Input: a multi-step arithmetic string.\n",
        "      Outputs:\n",
        "        1. sub_expressions: list of minimal binary sub-exprs solvable in parallel this round.\n",
        "        2. plan: concise instructions using placeholders (X1, X2, ...) to substitute results.\n",
        "      Rules:\n",
        "        - Only decompose the current expression, not future rounds.\n",
        "        - Carefully handle signs and parentheses.\n",
        "        - Maximize parallelizable binary ops.\n",
        "        - Use as few words as possible while describing the plan\n",
        "      \"\"\"\n",
        "    )\n",
        "    sig = ExpressionDecompositionSignature.with_instructions(self.decomp_instructions)\n",
        "    self.decomp_module = dspy.Predict(sig)\n",
        "    self.decomp_module.set_lm(self.llm)\n",
        "\n",
        "  def decompose(self, expr: str) -> tuple[list[str], str]:\n",
        "    result = self.decomp_module(full_expr=expr)\n",
        "    if self.verbose:\n",
        "      print(f\"[Decomposer] sub_expressions: {result.sub_expressions}\")\n",
        "      print(f\"[Decomposer] plan: {result.plan}\")\n",
        "    return result.sub_expressions, result.plan\n"
      ],
      "metadata": {
        "id": "FySd9VG2niWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. Expression Substitutor\n",
        "Merges numeric results back into the expression according to the plan."
      ],
      "metadata": {
        "id": "C7GrKI5BpeV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpressionSubstitutionSignature(dspy.Signature):\n",
        "  old_expr: str = dspy.InputField(desc=\"Previous expression.\")\n",
        "  plan: str = dspy.InputField(desc=\"Placeholder plan from decomposer.\")\n",
        "  partial_results: list[str] = dspy.InputField(desc=\"Results for each sub-expression.\")\n",
        "  new_expr: str = dspy.OutputField(desc=\"Updated expression or final binary op.\")\n",
        "  done: bool = dspy.OutputField(desc=\"True if only one operation remains.\")\n",
        "\n",
        "class ExpressionSubstitutor:\n",
        "  def __init__(self, llm, verbose=False):\n",
        "    self.llm = llm\n",
        "    self.verbose = verbose\n",
        "    self.subst_instructions = (\n",
        "      \"\"\"\n",
        "      You are a specialized expression aggregator.\n",
        "      Inputs:\n",
        "        - old_expr: expression from last round\n",
        "        - plan: how to substitute placeholders\n",
        "        - partial_results: values for each sub-expression\n",
        "      Outputs:\n",
        "        new_expr: updated expression after substitution\n",
        "        done: true if it's a single binary op\n",
        "      Steps:\n",
        "        1. Substitute results per plan, no new computation.\n",
        "        2. Set done = true if only one operator remains.\n",
        "        3. Remove redundant parentheses.\n",
        "      \"\"\"\n",
        "    )\n",
        "    sig = ExpressionSubstitutionSignature.with_instructions(self.subst_instructions)\n",
        "    self.subst_module = dspy.Predict(sig)\n",
        "    self.subst_module.set_lm(self.llm)\n",
        "\n",
        "  def substitute(self, old_expr: str, plan: str, partial_results: list[str]) -> tuple[str, bool]:\n",
        "    result = self.subst_module(\n",
        "      old_expr=old_expr,\n",
        "      plan=plan,\n",
        "      partial_results=partial_results\n",
        "    )\n",
        "    if self.verbose:\n",
        "      print(f\"[Substitutor] new_expr: {result.new_expr}, done: {result.done}\")\n",
        "    return result.new_expr, result.done"
      ],
      "metadata": {
        "id": "YFiBGce9pjh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3 Subproblem Solver Agent\n",
        "Solves a single binary arithmetic expression. Will be token-constrained."
      ],
      "metadata": {
        "id": "j-uYMtj6pmc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubproblemSignature(dspy.Signature):\n",
        "  sub_expr: str = dspy.InputField(desc=\"A minimal single-operator expression.\")\n",
        "  result: int = dspy.OutputField(desc=\"Integer result.\")\n",
        "\n",
        "class SubproblemAgent:\n",
        "  def __init__(self, idx: int, llm, verbose=False):\n",
        "    self.idx = idx\n",
        "    self.llm = llm\n",
        "    self.verbose = verbose\n",
        "    instr = (\n",
        "      \"\"\"\n",
        "      You are a specialized solver for one binary operation.\n",
        "      Input: '3 * -4' or '5 + 10'.\n",
        "      Output: the integer result.\n",
        "      \"\"\"\n",
        "    )\n",
        "    sig = SubproblemSignature.with_instructions(instr)\n",
        "    self.compute_module = dspy.Predict(sig)\n",
        "    self.compute_module.set_lm(self.llm)\n",
        "\n",
        "  def compute(self, sub_expr: str) -> str:\n",
        "    response = self.compute_module(sub_expr=sub_expr)\n",
        "    if self.verbose:\n",
        "      print(f\"[SubproblemAgent {self.idx}] {sub_expr} -> {response.result}\")\n",
        "    return str(response.result)\n"
      ],
      "metadata": {
        "id": "hN7Hg8XOppt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.4 Arithmetic Orchestrator\n",
        "Coordinates decomposer, subproblem agents, and substitutor until completion."
      ],
      "metadata": {
        "id": "Otp3yIF3p9Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerFormatterSignature(dspy.Signature):\n",
        "  final_expression: str = dspy.InputField(desc=\"Final binary operation.\")\n",
        "  answer: int = dspy.OutputField(desc=\"Integer answer.\")\n",
        "\n",
        "class ArithmeticOrchestrator:\n",
        "  def __init__(self, llm_decomp, llm_subst, llm_small, verbose=False):\n",
        "    self.decomposer = ExpressionDecomposer(llm_decomp, verbose)\n",
        "    self.substitutor = ExpressionSubstitutor(llm_subst, verbose)\n",
        "    self.llm_small = llm_small\n",
        "    self.answer_formatter = dspy.Predict(AnswerFormatterSignature)\n",
        "    self.answer_formatter.set_lm(llm_small)\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def solve_expression(self, expression: str) -> tuple[dspy.Prediction, list[str]]:\n",
        "    partial_exprs: list[str] = []\n",
        "    current_expr = expression\n",
        "    done = False\n",
        "    round_idx = 0\n",
        "\n",
        "    # Loop until the expression resolves to one binary operation\n",
        "    while not done:\n",
        "      if self.verbose:\n",
        "        print(f\"\\n=== Round {round_idx} ===\")\n",
        "        print(f\"Current expr: {current_expr}\")\n",
        "\n",
        "      partial_exprs.append(current_expr)\n",
        "\n",
        "      # 1) Decompose\n",
        "      sub_exprs, plan = self.decomposer.decompose(current_expr)\n",
        "      if not sub_exprs:\n",
        "        # Fully simplified\n",
        "        break\n",
        "\n",
        "      # 2) Parallel solve\n",
        "      partial_results = [\n",
        "        SubproblemAgent(i, self.llm_small, self.verbose).compute(sub)\n",
        "        for i, sub in enumerate(sub_exprs)\n",
        "      ]\n",
        "\n",
        "      # 3) Substitute\n",
        "      current_expr, done = self.substitutor.substitute(current_expr, plan, partial_results)\n",
        "      round_idx += 1\n",
        "\n",
        "    # Final answer formatting\n",
        "    final = self.answer_formatter(final_expression=current_expr)\n",
        "    if self.verbose:\n",
        "      print(f\"\\nFinal answer: {final.answer}\")\n",
        "    return final, partial_exprs"
      ],
      "metadata": {
        "id": "WqQgrIvGp2qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Usage"
      ],
      "metadata": {
        "id": "qrEPJ08DqWAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Configure models"
      ],
      "metadata": {
        "id": "L_scthzbqfPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4o3 = dspy.LM('openai/o3-mini-2025-01-31', temperature=1.0, max_tokens=20000, cache=False)\n",
        "gpt4o_mini_250_lotemp = dspy.LM('openai/gpt-4o-mini', temperature=0.1, max_tokens=250, cache=False)\n",
        "\n",
        "LLM_DECOMPOSE = gpt_4o3\n",
        "LLM_SUBSTITUTE = gpt_4o3\n",
        "LLM_SMALL = gpt4o_mini_250_lotemp"
      ],
      "metadata": {
        "id": "aOYrfN2LqgwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Single expression demo"
      ],
      "metadata": {
        "id": "CMVDw3I3qYCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orchestrator = ArithmeticOrchestrator(LLM_DECOMPOSE, LLM_SUBSTITUTE, LLM_SMALL, verbose=True)\n",
        "expr = \"((1 + 0 + 2 - 4) + (-9 + 6 * -5 + 8))\"\n",
        "final_ans, steps = orchestrator.solve_expression(expr)\n",
        "print(f\"Answer: {final_ans.answer}\")\n",
        "\n",
        "# Baseline on same example\n",
        "o3_reasoning_agent = dspy.Predict(\"input -> reasoning, answer\")\n",
        "o3_reasoning_agent.set_lm(gpt_4o3)\n",
        "response = o3_reasoning_agent(input=expr)\n",
        "print(response.reasoning)"
      ],
      "metadata": {
        "id": "U175YYJyqaX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Batch Evaluation\n",
        "**Note:** This will take a LONG time to run"
      ],
      "metadata": {
        "id": "wjJXT64UrO0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalset = multi_step_combined[10:210]\n",
        "correct = []\n",
        "for ex in evalset:\n",
        "  ans, _ = orchestrator.solve_expression(ex['input'])\n",
        "  correct.append(meta_prompter_metric(ex, ans))\n",
        "print(f\"Batch accuracy: {np.mean(correct) * 100:.2f}%\")\n",
        "\n",
        "### Baseline comparison\n",
        "o3_reasoning_agent = dspy.Predict(\"input -> reasoning, answer\")\n",
        "o3_reasoning_agent.set_lm(gpt_4o3)\n",
        "evaluate = dspy.Evaluate(devset=evalset, metric=meta_prompter_metric)\n",
        "res = evaluate(o3_reasoning_agent)\n",
        "print(f\"Baseline batch accuracy: {res['score']:.2f}%\")"
      ],
      "metadata": {
        "id": "x4KZZ-8wrTDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}